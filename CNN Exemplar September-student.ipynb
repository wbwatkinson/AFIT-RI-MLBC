{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variant Machine Learning Problem    September 2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we move into the convolutional NN world with another fairly simple exemplar and a few new concepts to add to the mix.  This is a more individual notebook, and we'll get back to group interactions soon.\n",
    "\n",
    "You can add visualization and data checking as in previous notebooks if you like, to get a sense of what the data is doing.  We are using a larger database of images and doing a bit of processing to get them ready for ingestion into the model.  The three tasks to be done are highlighted, and the four questions have empty markdown blocks underneath for you to enter your answers.  To answer the questions, just double-click on the \"Your answer\" text in the cell area and enter your answers.  Remember to save your final notebook and submit during/after our 1500 session. (Files/Part 1/Submitted Assignments Part1/Week 4../your folder)\n",
    "\n",
    "The dataset will take a bit to download if you have a slow internet connection (but only the first time).  Things should work along without any errors.  Please don't wait until 2:00 pm to see if this runs -- check it out and then have lunch!  You should be spending about three hours on this to get the concepts down if you don't have problems.  Check first... \n",
    "\n",
    "Depending on your computer, it might take some time to do the training for larger numbers of epochs as you change parameters, do some timing runs to verify.  Another good reason to start early!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib, os\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is retrieved by a keras utility that understands\n",
    "# stored versions, including the one we are working on - Fashion MNIST\n",
    "# (reference here: https://keras.io/api/datasets/fashion_mnist/)\n",
    "# It is the \"fashion\" corollary to MNIST - 60,000 train/10,000 test\n",
    "# grayscale images of 10 types of clothing.\n",
    "\n",
    "fMNIST_data = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the train/test data, it is already segmented\n",
    "(X_train, y_train), (X_test, y_test) = fMNIST_data.load_data()\n",
    "\n",
    "# X_train[0] if you want to see the 28x28 pixel component of the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure everything is correct\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# take advantage of the 'imshow()' method of matplotlib to check one out\n",
    "plt.imshow(X_train[0], cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">Question 1:</p>  Why is it an inverse image; shouldn't the background be white and the higher values represent darker tones? (*hint - print out the pixel values, as was done in the MNIST notebook, and think about processing the values*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">Your Answer 1:</p>  ?? (double-click here and edit...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also a good time to re-iterate how NN's don't \"see\" like you and I do. The representation of the images is 28x28, and that is the world it knows.  How well would you do with the identification of some of the clothing objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as the simple NN earlier, need to normalize the data between zero\n",
    "# and one to help with calculations/normalization.  You could try without this\n",
    "# to see what impact it has, but you'd still have to convert to floats\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add a dimension to the array for the one grayscale channel,\n",
    "# then we're ready to build the model\n",
    "X_train = X_train.reshape((60000, 28, 28, 1))\n",
    "X_test = X_test.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">Question 1:</p>  Why do we add a dimension to the array, when it is already appropriate for display? (*silly hint -- you could comment the previous two lines out and see what happens... it does give you a little insight*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">Question 1:</p>  ?? (double-click here and edit...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also set up the categorical encoding for the y values as:\n",
    "#y_train = tf.keras.utils.to_categorical(y_train)\n",
    "#y_test = tf.keras.utils.to_categorical(y_test)\n",
    "# as we have in previous work.  But we don't here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">Question 3:</p>  Why don't we use straight \"categorical_crossentropy\" as the loss function in the model below? (*hint:  it has something to do with larger and larger datasets/labelsets and scaling of compute, not just this notebook*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">Your answer 3:</p>  ?? (double-click here and edit...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the network we are going to build.  If you recall from the introductory session those two long weeks ago, we looked at three basic types of NN architecture; Fully Connected (called Dense now...), Convolutional, and Recurrent.  The reason for choosing those three was to highlight what foundational basis they had for \"solving problems.\"  \n",
    "1. The FC network is pretty good for most things that need NN solutions where lots of basic matrix/vector manipulations (regression, \"prediction\", classification based strictly on column vectors of sample features).  \n",
    "2. Convolution is based on the foundational idea that there is a need to take *extent* into account -- where things are in relation to other things in a \"snapshot\" distance sense (like 2D pictures or other higher-dimension object-spaces).  \n",
    "3. Recurrent is based on the idea that there is a need to take *sequence* into account, be it in the sense of a time series of digitized signal measurements, or a sequence of written or spoken words where the correlation between data samples requires a memory aspect associated with before/after relationships.\n",
    "The purpose in selecting these three was not to constrain our work to these specific models, but to understand the reasoning underneath the models and extend that reasoning to newer architectural approaches that are solving the same class of problems.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick graphic on some of the dimensional manipulations for the first convolutional layer that might help -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This graphic is not the best; it fits here because the cohort has already seen this process in a slide brief and talked through it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/conv2d.jpg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "Below is the model build for the Convolutional NN.  Find out how it performs.  Experiment with each of the activation functions, the loss functions, and the optimizers in the list below.  Don't modify other aspects just yet.  This code runs fairly quickly with low numbers of epochs, try various combinations.  It is easy to get better results, harder to get \"almost\" 100%. \n",
    "1.  Activation functions: sigmoid, tanh, relu; note the relu to start.  Probably the best choice,  but what would you choose and why?\n",
    "2.  Loss functions: mean squared error, binary cross entropy, categorical cross entropy.  \n",
    "3.  Optimizers: SGD, Adam.  Here again SGD is good to start things going, what does Adam do for you?  There is a default learning rate for Adam, but you can change it with a parameter setting.  Try a couple to get a little better result.\n",
    "\n",
    "**Task 2**\n",
    "Add a second convolutional layer and see if you can get a little closer to the 100% mark.  Try comparing fewer epochs with the additional layer and see if it correlates with the single Conv2D with a few more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a sequential model that is much more complex than yesterady\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# add the first layer (it has to match the input features - the 'input shape'\n",
    "# below - they are 28x28 scaled pixel values)\n",
    "model.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 2,\n",
    "                                 padding = 'same', activation = 'relu',\n",
    "                                 input_shape = (28,28,1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size = 2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "# another \"layer set\" for Conv2d goes here  - Task 2\n",
    "\n",
    "# After the convolution layer(s), we need to get back to a linear sample\n",
    "model.add(tf.keras.layers.Flatten()) #(note no 'parameters' at all)\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "# and the final output layer\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# select an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "# select a loss function\n",
    "loss = 'sparse_categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics =['accuracy'])\n",
    "\n",
    "# view a summmary of the model (textual summary) showing details\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">Question 4:</p>  Can you break down the calculation of parameters *and the output shape* above?  You should have a head start, with the dense layer (from last week).  Just need to ponder the convolutional layer, and where the weights are, and the flattening.  Not magic, just math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red\">Your answer 4:</p>  ?? (double-click here and edit...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructor answer:**  *320 = 2 * 2 * 64 [filter wxh times num_filters] plus 64 (bias); flatten is just 14 * 14 * 64.  Dense 2 is 12544 * 256 + 256.  Final is 256 * 10 + 10.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a visual representation of the model using a keras utility\n",
    "# You also need to have pydot and graphvis installed for this to work\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fMNIST_model = model.fit(X_train, y_train, batch_size=64, epochs=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"\\nLoss, accuracy on test data: \")\n",
    "print(\"%0.4f %0.2f%%\" % (eval[0],eval[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "Show output on a sequence of 10 images in the test dataset and whether or not they were correct.  Your choice of method - if you are time constrained, just use text and show that it is (mostly) as you would expect given the accuracy results you obtained above.  If you want to exercise the multiple sub-figure plotting (from previous notebook, or your own), you can certainly do that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
